{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n\nimport io # Input/Output Module\nimport os # OS interfaces\nimport cv2 # OpenCV package\nimport numpy as np # linear algebra\nimport keras\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom urllib import request # module for opening HTTP requests\nfrom matplotlib import pyplot as plt # Plotting library","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.230891,"end_time":"2021-03-08T07:57:06.335029","exception":false,"start_time":"2021-03-08T07:57:06.104138","status":"completed"},"tags":[],"id":"lxF9ebw_cpZJ","execution":{"iopub.status.busy":"2022-04-03T18:30:30.571363Z","iopub.execute_input":"2022-04-03T18:30:30.571718Z","iopub.status.idle":"2022-04-03T18:30:37.986625Z","shell.execute_reply.started":"2022-04-03T18:30:30.571632Z","shell.execute_reply":"2022-04-03T18:30:37.985477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"width:100%; height:140px\">\n    <img src=\"https://www.kuleuven.be/internationaal/thinktank/fotos-en-logos/ku-leuven-logo.png/image_preview\" width = 300px, heigh = auto align=left>\n</div>\n\n\nKUL H02A5a Computer Vision: Group Assignment 1\n---------------------------------------------------------------\nStudent numbers: <span style=\"color:red\">r0703889, r0909802, r0716758, r0916443, r0822692</span>.\n\nThe goal of this assignment is to explore more advanced techniques for constructing features that better describe objects of interest and to perform face recognition using these features. This assignment will be delivered in groups of 5 (either composed by you or randomly assigned by your TA's).\n\nIn this assignment you are a group of computer vision experts that have been invited to ECCV 2021 to do a tutorial about  \"Feature representations, then and now\". To prepare the tutorial you are asked to participate in a kaggle competition and to release a notebook that can be easily studied by the tutorial participants. Your target audience is: (master) students who want to get a first hands-on introduction to the techniques that you apply.\n\n---------------------------------------------------------------\nThis notebook is structured as follows:\n0. Data loading & Preprocessing\n1. Feature Representations\n2. Evaluation Metrics \n3. Classifiers\n4. Experiments\n5. Publishing best results\n6. Discussion\n\nMake sure that your notebook is **self-contained** and **fully documented**. Walk us through all steps of your code. Treat your notebook as a tutorial for students who need to get a first hands-on introduction to the techniques that you apply. Provide strong arguments for the design choices that you made and what insights you got from your experiments. Make use of the *Group assignment* forum/discussion board on Toledo if you have any questions.\n\nFill in your student numbers above and get to it! Good luck! \n\n\n<div class=\"alert alert-block alert-info\">\n<b>NOTE:</b> This notebook is just a example/template, feel free to adjust in any way you please! Just keep things organised and document accordingly!\n</div>\n\n<div class=\"alert alert-block alert-info\">\n<b>NOTE:</b> Clearly indicate the improvements that you make!!! You can for instance use titles like: <i>3.1. Improvement: Non-linear SVM with RBF Kernel.<i>\n</div>\n    \n---------------------------------------------------------------\n# 0. Data loading & Preprocessing\n\n## 0.1. Loading data\nThe training set is many times smaller than the test set and this might strike you as odd, however, this is close to a real world scenario where your system might be put through daily use! In this session we will try to do the best we can with the data that we've got! ","metadata":{"papermill":{"duration":0.022868,"end_time":"2021-03-08T07:57:06.382109","exception":false,"start_time":"2021-03-08T07:57:06.359241","status":"completed"},"tags":[],"id":"Q0aKDUaocpZO"}},{"cell_type":"code","source":"os.listdir('/kaggle/input/computer-vision-assignment/kul-h02a5a-computer-vision-ga1-2022')","metadata":{"id":"0D3jLRmB8wE8","execution":{"iopub.status.busy":"2022-04-03T18:30:37.988253Z","iopub.execute_input":"2022-04-03T18:30:37.988479Z","iopub.status.idle":"2022-04-03T18:30:38.005829Z","shell.execute_reply.started":"2022-04-03T18:30:37.988453Z","shell.execute_reply":"2022-04-03T18:30:38.004745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Input data files are available in the read-only \"../input/\" directory\n\ntrain = pd.read_csv(\n    '/kaggle/input/computer-vision-assignment/kul-h02a5a-computer-vision-ga1-2022/train_set.csv', index_col = 0)\ntrain.index = train.index.rename('id')\n\ntest = pd.read_csv(\n    '/kaggle/input/computer-vision-assignment/kul-h02a5a-computer-vision-ga1-2022/test_set.csv', index_col = 0)\ntest.index = test.index.rename('id')\n\n# read the images as numpy arrays and store in \"img\" column\ntrain['img'] = [cv2.cvtColor(np.load('/kaggle/input/computer-vision-assignment/kul-h02a5a-computer-vision-ga1-2022/train/train_{}.npy'.format(index), allow_pickle=False), cv2.COLOR_BGR2RGB) \n                for index, row in train.iterrows()]\n\ntest['img'] = [cv2.cvtColor(np.load('/kaggle/input/computer-vision-assignment/kul-h02a5a-computer-vision-ga1-2022/test/test_{}.npy'.format(index), allow_pickle=False), cv2.COLOR_BGR2RGB) \n                for index, row in test.iterrows()]\n  \n\ntrain_size, test_size = len(train),len(test)\n\n\"The training set contains {} examples, the test set contains {} examples.\".format(train_size, test_size)","metadata":{"papermill":{"duration":37.543619,"end_time":"2021-03-08T07:57:43.9495","exception":false,"start_time":"2021-03-08T07:57:06.405881","status":"completed"},"tags":[],"id":"HJTeDamCcpZQ","execution":{"iopub.status.busy":"2022-04-03T18:30:38.007656Z","iopub.execute_input":"2022-04-03T18:30:38.00821Z","iopub.status.idle":"2022-04-03T18:31:04.354691Z","shell.execute_reply.started":"2022-04-03T18:30:38.008162Z","shell.execute_reply":"2022-04-03T18:31:04.353289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Note: this dataset is a subset of the* [*VGG face dataset*](https://www.robots.ox.ac.uk/~vgg/data/vgg_face/).\n\n## 0.2. A first look\nLet's have a look at the data columns and class distribution.","metadata":{"papermill":{"duration":0.023377,"end_time":"2021-03-08T07:57:43.997466","exception":false,"start_time":"2021-03-08T07:57:43.974089","status":"completed"},"tags":[],"id":"Evc-Sw1HcpZR"}},{"cell_type":"code","source":"# The training set contains an identifier, name, image information and class label\ntrain.head()","metadata":{"papermill":{"duration":3.315629,"end_time":"2021-03-08T07:57:47.336913","exception":false,"start_time":"2021-03-08T07:57:44.021284","status":"completed"},"tags":[],"id":"OcumKS_6cpZS","execution":{"iopub.status.busy":"2022-04-03T18:31:04.356092Z","iopub.execute_input":"2022-04-03T18:31:04.356783Z","iopub.status.idle":"2022-04-03T18:31:21.464658Z","shell.execute_reply.started":"2022-04-03T18:31:04.356741Z","shell.execute_reply":"2022-04-03T18:31:21.463448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The test set only contains an identifier and corresponding image information.\n\ntest.head(1)","metadata":{"papermill":{"duration":3.283501,"end_time":"2021-03-08T07:57:50.644778","exception":false,"start_time":"2021-03-08T07:57:47.361277","status":"completed"},"tags":[],"id":"Hvn_4dumcpZS","outputId":"8a82ec11-b337-41ec-adb5-5e183d34b00f","execution":{"iopub.status.busy":"2022-04-03T18:31:21.467042Z","iopub.execute_input":"2022-04-03T18:31:21.467322Z","iopub.status.idle":"2022-04-03T18:31:24.801446Z","shell.execute_reply.started":"2022-04-03T18:31:21.467287Z","shell.execute_reply":"2022-04-03T18:31:24.799817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The class distribution in the training set:\ntrain.groupby('name').agg({'img':'count', 'class': 'max'})","metadata":{"papermill":{"duration":0.046628,"end_time":"2021-03-08T07:57:50.716317","exception":false,"start_time":"2021-03-08T07:57:50.669689","status":"completed"},"tags":[],"id":"7e87e7u-cpZS","outputId":"523b08a1-588e-462b-eb01-b00aedb2830d","execution":{"iopub.status.busy":"2022-04-03T18:31:24.802921Z","iopub.execute_input":"2022-04-03T18:31:24.80344Z","iopub.status.idle":"2022-04-03T18:31:24.822715Z","shell.execute_reply.started":"2022-04-03T18:31:24.803389Z","shell.execute_reply":"2022-04-03T18:31:24.821819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note that **Jesse is assigned the classification label 1**, and **Mila is assigned the classification label 2**. The dataset also contains 20 images of **look alikes (assigned classification label 0)** and the raw images. \n\n## 0.3. Preprocess data\n### 0.3.1 Example: HAAR face detector\nIn this example we use the [HAAR feature based cascade classifiers](https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_objdetect/py_face_detection/py_face_detection.html) to detect faces, then the faces are resized so that they all have the same shape. If there are multiple faces in an image, we only take the first one. \n\n<div class=\"alert alert-block alert-info\"> <b>NOTE:</b> You can write temporary files to <code>/kaggle/temp/</code> or <code>../../tmp</code>, but they won't be saved outside of the current session\n</div>\n","metadata":{"papermill":{"duration":0.025108,"end_time":"2021-03-08T07:57:50.766719","exception":false,"start_time":"2021-03-08T07:57:50.741611","status":"completed"},"tags":[],"id":"5LQYC2mXcpZT"}},{"cell_type":"code","source":"class HAARPreprocessor():\n    \"\"\"Preprocessing pipeline built around HAAR feature based cascade classifiers. \"\"\"\n    \n    def __init__(self, path, face_size):\n        self.face_size = face_size\n        file_path = os.path.join(path, \"haarcascade_frontalface_default.xml\")\n        if not os.path.exists(file_path): \n            if not os.path.exists(path):\n                os.mkdir(path)\n            self.download_model(file_path)\n        \n        self.classifier = cv2.CascadeClassifier(file_path)\n  \n    def download_model(self, path):\n        url = \"https://raw.githubusercontent.com/opencv/opencv/master/data/\"\\\n            \"haarcascades/haarcascade_frontalface_default.xml\"\n        \n        with request.urlopen(url) as r, open(path, 'wb') as f:\n            f.write(r.read())\n            \n    def detect_faces(self, img):\n        \"\"\"Detect all faces in an image.\"\"\"\n        \n        img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n        return self.classifier.detectMultiScale(\n            img_gray,\n            scaleFactor=1.2,\n            minNeighbors=5,\n            minSize=(30, 30),\n            flags=cv2.CASCADE_SCALE_IMAGE\n        )\n        \n    def extract_faces(self, img):\n        \"\"\"Returns all faces (cropped) in an image.\"\"\"\n        \n        faces = self.detect_faces(img)\n\n        return [img[y:y+h, x:x+w] for (x, y, w, h) in faces]\n    \n    def preprocess(self, data_row):\n        faces = self.extract_faces(data_row['img'])\n        \n        # if no faces were found, return None\n        if len(faces) == 0:\n            nan_img = np.empty(self.face_size + (3,))\n            nan_img[:] = np.nan\n            return nan_img\n        \n        # only return the first face\n        return cv2.resize(faces[0], self.face_size, interpolation = cv2.INTER_AREA)\n            \n    def __call__(self, data):\n        return np.stack([self.preprocess(row) for _, row in data.iterrows()]).astype(int)","metadata":{"papermill":{"duration":0.042776,"end_time":"2021-03-08T07:57:50.834913","exception":false,"start_time":"2021-03-08T07:57:50.792137","status":"completed"},"tags":[],"id":"OdjkKuNlcpZU","execution":{"iopub.status.busy":"2022-04-03T18:31:24.824278Z","iopub.execute_input":"2022-04-03T18:31:24.825164Z","iopub.status.idle":"2022-04-03T18:31:24.844958Z","shell.execute_reply.started":"2022-04-03T18:31:24.825115Z","shell.execute_reply":"2022-04-03T18:31:24.843914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Visualise**\n\nLet's plot a few examples.","metadata":{"papermill":{"duration":0.025332,"end_time":"2021-03-08T07:57:50.885849","exception":false,"start_time":"2021-03-08T07:57:50.860517","status":"completed"},"tags":[],"id":"ZWUoaxjzcpZU"}},{"cell_type":"code","source":"# parameter to play with \nFACE_SIZE = (100, 100)\n\ndef plot_image_sequence(data, n, imgs_per_row=7):\n    n_rows = 1 + int(n/(imgs_per_row+1))\n    n_cols = min(imgs_per_row, n)\n\n    f,ax = plt.subplots(n_rows,n_cols, figsize=(10*n_cols,10*n_rows))\n    for i in range(n):\n        if n == 1:\n            ax.imshow(data[i])\n        elif n_rows > 1:\n            ax[int(i/imgs_per_row),int(i%imgs_per_row)].imshow(data[i])\n        else:\n            ax[int(i%n)].imshow(data[i])\n    plt.show()\n\n    \n#preprocessed data \npreprocessor = HAARPreprocessor(path = '../../tmp', face_size=FACE_SIZE)\n\ntrain_X, train_y = preprocessor(train), train['class'].values\ntest_X = preprocessor(test)\n\n","metadata":{"papermill":{"duration":62.263517,"end_time":"2021-03-08T07:58:53.174859","exception":false,"start_time":"2021-03-08T07:57:50.911342","status":"completed"},"tags":[],"id":"sbpu5ch7cpZV","execution":{"iopub.status.busy":"2022-04-03T18:31:24.846239Z","iopub.execute_input":"2022-04-03T18:31:24.846552Z","iopub.status.idle":"2022-04-03T18:32:28.26107Z","shell.execute_reply.started":"2022-04-03T18:31:24.846496Z","shell.execute_reply":"2022-04-03T18:32:28.260005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot faces of Michael and Sarah\n\nplot_image_sequence(train_X[train_y == 0], n=20, imgs_per_row=10)","metadata":{"papermill":{"duration":2.635787,"end_time":"2021-03-08T07:58:55.836611","exception":false,"start_time":"2021-03-08T07:58:53.200824","status":"completed"},"tags":[],"id":"iksq0C8_cpZV","outputId":"02c1d0d8-2366-480e-bac9-f8523f3dcc2e","execution":{"iopub.status.busy":"2022-04-03T18:32:28.262339Z","iopub.execute_input":"2022-04-03T18:32:28.262588Z","iopub.status.idle":"2022-04-03T18:32:30.897721Z","shell.execute_reply.started":"2022-04-03T18:32:28.262546Z","shell.execute_reply":"2022-04-03T18:32:30.896663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot faces of Jesse\n\nplot_image_sequence(train_X[train_y == 1], n=30, imgs_per_row=10)","metadata":{"papermill":{"duration":3.840961,"end_time":"2021-03-08T07:58:59.72249","exception":false,"start_time":"2021-03-08T07:58:55.881529","status":"completed"},"tags":[],"id":"ga3FKp48cpZW","outputId":"c6c73511-9956-4dd5-f0e5-6033538d5a69","execution":{"iopub.status.busy":"2022-04-03T18:32:30.899109Z","iopub.execute_input":"2022-04-03T18:32:30.899432Z","iopub.status.idle":"2022-04-03T18:32:35.037318Z","shell.execute_reply.started":"2022-04-03T18:32:30.899387Z","shell.execute_reply":"2022-04-03T18:32:35.036373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot faces of Mila\n\nplot_image_sequence(train_X[train_y == 2], n=30, imgs_per_row=10)","metadata":{"papermill":{"duration":3.910256,"end_time":"2021-03-08T07:59:03.703299","exception":false,"start_time":"2021-03-08T07:58:59.793043","status":"completed"},"tags":[],"id":"rW6wUU6ocpZW","outputId":"e7af31b2-ee19-4939-94ff-ef7d4eecb581","execution":{"iopub.status.busy":"2022-04-03T18:32:35.038786Z","iopub.execute_input":"2022-04-03T18:32:35.039043Z","iopub.status.idle":"2022-04-03T18:32:39.1166Z","shell.execute_reply.started":"2022-04-03T18:32:35.039011Z","shell.execute_reply":"2022-04-03T18:32:39.115548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 0.4. Store Preprocessed data (optional)\n<div class=\"alert alert-block alert-info\">\n<b>NOTE:</b> You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\". Feel free to use this to store intermediary results.\n</div>","metadata":{"papermill":{"duration":0.100995,"end_time":"2021-03-08T07:59:03.904684","exception":false,"start_time":"2021-03-08T07:59:03.803689","status":"completed"},"tags":[],"id":"PSXBCTa7cpZW"}},{"cell_type":"code","source":"# save preprocessed data\n# prep_path = '/kaggle/working/prepped_data/'\n# if not os.path.exists(prep_path):\n#     os.mkdir(prep_path)\n    \n# np.save(os.path.join(prep_path, 'train_X.npy'), train_X)\n# np.save(os.path.join(prep_path, 'train_y.npy'), train_y)\n# np.save(os.path.join(prep_path, 'test_X.npy'), test_X)\n\n# load preprocessed data\n# prep_path = '/kaggle/working/prepped_data/'\n# if not os.path.exists(prep_path):\n#     os.mkdir(prep_path)\n# train_X = np.load(os.path.join(prep_path, 'train_X.npy'))\n# train_y = np.load(os.path.join(prep_path, 'train_y.npy'))\n# test_X = np.load(os.path.join(prep_path, 'test_X.npy'))","metadata":{"papermill":{"duration":0.109823,"end_time":"2021-03-08T07:59:04.11528","exception":false,"start_time":"2021-03-08T07:59:04.005457","status":"completed"},"tags":[],"id":"JgbVeH4VcpZX","execution":{"iopub.status.busy":"2022-04-03T18:32:39.11799Z","iopub.execute_input":"2022-04-03T18:32:39.118827Z","iopub.status.idle":"2022-04-03T18:32:39.123992Z","shell.execute_reply.started":"2022-04-03T18:32:39.118774Z","shell.execute_reply":"2022-04-03T18:32:39.123258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we are ready to rock!","metadata":{"papermill":{"duration":0.100101,"end_time":"2021-03-08T07:59:04.315571","exception":false,"start_time":"2021-03-08T07:59:04.21547","status":"completed"},"tags":[],"id":"us_JT9ipcpZX"}},{"cell_type":"markdown","source":"# 1. Feature Representations\n## 1.0. Example: Identify feature extractor\nOur example feature extractor doesn't actually do anything... It just returns the input:\n$$\n\\forall x : f(x) = x.\n$$\n\nIt does make for a good placeholder and baseclass ;).","metadata":{"papermill":{"duration":0.100212,"end_time":"2021-03-08T07:59:04.516059","exception":false,"start_time":"2021-03-08T07:59:04.415847","status":"completed"},"tags":[],"id":"rVVB49W7cpZX"}},{"cell_type":"code","source":"class IdentityFeatureExtractor:\n    \"\"\"A simple function that returns the input\"\"\"\n    \n    def transform(self, X):\n        return X\n    \n    def __call__(self, X):\n        return self.transform(X)","metadata":{"papermill":{"duration":0.108781,"end_time":"2021-03-08T07:59:04.725071","exception":false,"start_time":"2021-03-08T07:59:04.61629","status":"completed"},"tags":[],"id":"oV-Y4lBzcpZX","execution":{"iopub.status.busy":"2022-04-03T18:54:02.430856Z","iopub.execute_input":"2022-04-03T18:54:02.431246Z","iopub.status.idle":"2022-04-03T18:54:02.436983Z","shell.execute_reply.started":"2022-04-03T18:54:02.431139Z","shell.execute_reply":"2022-04-03T18:54:02.43589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Preprocessing:\n    \"\"\"Preprocessing of data for future\"\"\"\n    \n    def transform(self, X):\n        nb_train_images = 80\n        M = np.array(np.zeros([nb_train_images,10000]))\n        for n in range(nb_train_images):\n            Im = train_X[train_y][n]\n            Im = rgb2gray(Im)\n            A = np.asarray(Im).reshape(-1)\n            M[n,:] = A\n            return M\n        \n        \n    def __call__(self, X):\n        return self.transform(X)","metadata":{"execution":{"iopub.status.busy":"2022-04-03T18:54:49.971311Z","iopub.execute_input":"2022-04-03T18:54:49.971616Z","iopub.status.idle":"2022-04-03T18:54:49.981998Z","shell.execute_reply.started":"2022-04-03T18:54:49.971583Z","shell.execute_reply":"2022-04-03T18:54:49.980893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.1. Baseline 1: HOG feature extractor/Scale Invariant Feature Transform\n\nHOG and SIFT are both very similar feature descriptors. \nHowever, the HOG feature extractor is used more frequently in classification problems than SIFT. The latter is used more in identificating specific objects. By this reason, we decided to implement the HOG feature extractor.\n\n\nThe HOG feature extractor counts occurences of gradient orientations in a localized portion of an image. The purpose of this technique to receive a better objects detection by focusing on the structure/shape of an object. This technique is widely used in computer vision and image processing, and was introducedin a research paper of Dala and Triggs in 2005.\nThis technique returns mostly sligtly better results than other edge descriptors because it also uses the magnitude and angle gradient to compute features, from which it generates an histogram.\n","metadata":{"papermill":{"duration":0.134288,"end_time":"2021-03-08T07:59:04.959911","exception":false,"start_time":"2021-03-08T07:59:04.825623","status":"completed"},"tags":[],"id":"fYQ6D7ShcpZY"}},{"cell_type":"code","source":"#importing required libraries\nfrom skimage.io import imread\nfrom skimage.transform import resize\nfrom skimage.feature import hog\nfrom skimage import exposure\nfrom skimage import data\nfrom skimage.color import (rgb2gray)\nimport matplotlib.pyplot as plt","metadata":{"id":"AE2XC17kbrVy","execution":{"iopub.status.busy":"2022-04-03T18:32:39.143236Z","iopub.execute_input":"2022-04-03T18:32:39.144035Z","iopub.status.idle":"2022-04-03T18:32:40.556461Z","shell.execute_reply.started":"2022-04-03T18:32:39.143994Z","shell.execute_reply":"2022-04-03T18:32:40.555534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class HOGFeatureExtractor(IdentityFeatureExtractor):\n    \"\"\"\n    https://www.thepythoncode.com/article/hog-feature-extraction-in-python\n\n    \"\"\"\n    \n    def __init__(self, X):\n        self.X = X\n\n    def save_extract(self):\n        \"\"\"\n         TODO: DOES NOT EXPORT ANYMORE - PROBLEM!! \n         \n         Outputs the images converted by HOG extractor\n         \n         Returns\n         -------\n         list_img : Array of arrays representing the Hog_images\n        \"\"\"\n        n = train_X[train_y].shape\n        imgs_per_row = 10\n        n_rows = 1 + int(n[0]/(imgs_per_row+1))\n        n_cols = min(imgs_per_row, n[0])\n        list_img = []\n        \n        for i in range(n[0]):\n            img = train_X[train_y][i]\n            fd, hog_image = hog(img, orientations=10, pixels_per_cell=(5, 5),  cells_per_block=(2, 2), visualize=True, multichannel=True)\n        \n            prep_path = '/kaggle/working/prepped_data/'\n            if not os.path.exists(prep_path):\n                os.mkdir(prep_path)\n            \n            np.save(os.path.join(prep_path, 'train_X_HOG_{}.npy'.format(i)), hog_image)\n            list_img.append(hog_image)\n        \n        return list_img\n\n    def plot_extract(self):\n        n = train_X[train_y].shape\n        imgs_per_row = 10\n        n_rows = 1 + int(n[0]/(imgs_per_row+1))\n        n_cols = min(imgs_per_row, n[0])\n        list_img = []\n\n        for i in range(n[0]):\n            img = train_X[train_y][i]\n            fd, hog_image = hog(img, orientations=10, pixels_per_cell=(5, 5), cells_per_block=(2, 2), visualize=True, multichannel=True)\n            list_img.append(hog_image)\n\n        # creating hog features\n        f,ax = plt.subplots(n_rows,n_cols, figsize=(10*n_cols,10*n_rows))\n\n        for i in range(n[0]):\n            if n[0] == 1:\n                ax.imshow(list_img[i], cmap=\"gray\")\n            elif n_rows > 1:\n                ax[int(i/imgs_per_row),int(i%imgs_per_row)].imshow(list_img[i], cmap=\"gray\")\n            else:\n                ax[int(i%n[0])].imshow(list_img[i], cmap=\"gray\")\n\n        plt.show()  \n\n\n    def transform(self, X):\n        raise NotImplmentedError","metadata":{"papermill":{"duration":0.110122,"end_time":"2021-03-08T07:59:05.171171","exception":false,"start_time":"2021-03-08T07:59:05.061049","status":"completed"},"tags":[],"id":"qfmnEHZtcpZY","execution":{"iopub.status.busy":"2022-04-03T18:32:40.557946Z","iopub.execute_input":"2022-04-03T18:32:40.558205Z","iopub.status.idle":"2022-04-03T18:32:40.577502Z","shell.execute_reply.started":"2022-04-03T18:32:40.558175Z","shell.execute_reply":"2022-04-03T18:32:40.5762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hog_model = HOGFeatureExtractor(4)\n\n# Save the extracted data to /kaggle/working\nhog_model.save_extract()\n\n\n# plot the extracted data as hog\nhog_model.plot_extract()","metadata":{"id":"xUDB9wzDasja","outputId":"0eeb2d66-5219-4484-8b4d-ddbb74f4d0bd","execution":{"iopub.status.busy":"2022-04-03T18:32:40.578844Z","iopub.execute_input":"2022-04-03T18:32:40.579098Z","iopub.status.idle":"2022-04-03T18:33:09.792694Z","shell.execute_reply.started":"2022-04-03T18:32:40.579067Z","shell.execute_reply":"2022-04-03T18:33:09.791894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.1.1. t-SNE Plots\nt-Distributed Stochastic Neighbor Embedding (t-SNE) is not a mathematical technique as PCA, but a probabilistic one. It minimizes the divergence between two distributions: a distribution that measures pairwise similarities of the input objects and a distribution that measures pairwise similarities of the corresponding low-dimensional points in the embedding [HERSCHRIJVEN - COPYRIGHT]","metadata":{"papermill":{"duration":0.100377,"end_time":"2021-03-08T07:59:05.372401","exception":false,"start_time":"2021-03-08T07:59:05.272024","status":"completed"},"tags":[],"id":"4HH01BUVcpZY"}},{"cell_type":"code","source":"import time\nimport numpy as np\nimport pandas as pd\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nimport seaborn as sns","metadata":{"id":"bKmpat_h2Z32","execution":{"iopub.status.busy":"2022-04-03T18:33:09.793757Z","iopub.execute_input":"2022-04-03T18:33:09.794742Z","iopub.status.idle":"2022-04-03T18:33:10.401299Z","shell.execute_reply.started":"2022-04-03T18:33:09.794694Z","shell.execute_reply":"2022-04-03T18:33:10.400318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save preprocessed data\nlist_img = hog_model.save_extract()\n\nprep_path = '/kaggle/working/prepped_data/'\nif not os.path.exists(prep_path):\n     os.mkdir(prep_path)\n    \nnp.save(os.path.join(prep_path, 'train_X_HOG.npy'), list_img)\n#np.save(os.path.join(prep_path, 'train_y.npy'), train_y)\n#np.save(os.path.join(prep_path, 'test_X.npy'), test_X)","metadata":{"id":"kwxvsmLpOMuu","outputId":"7f3e5cb4-50cf-4462-b33b-ce3008459e32","execution":{"iopub.status.busy":"2022-04-03T18:33:10.402745Z","iopub.execute_input":"2022-04-03T18:33:10.40301Z","iopub.status.idle":"2022-04-03T18:33:19.644133Z","shell.execute_reply.started":"2022-04-03T18:33:10.402977Z","shell.execute_reply":"2022-04-03T18:33:19.643367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dataset\n# https://towardsdatascience.com/visualising-high-dimensional-datasets-using-pca-and-t-sne-in-python-8ef87e7915b\n\n# Attach img to dataframe\n#train.head()\n#train.index = train.index.rename('id')\n\n#train['img_HOG'] = [cv2.cvtColor(np.load('/content/drive/MyDrive/kaggle/working/prepped_data/train_X_HOG_{}.npy'.format(index), allow_pickle=False), cv2.COLOR_BGR2RGB) \n#                for index, row in train.iterrows()]\n\n\ntrain['img_hog'] = [np.load('/kaggle/working/prepped_data/train_X_HOG_{}.npy'.format(index), allow_pickle=False) \n                for index, row in train.iterrows()]\n\n\ntrain.head()\n","metadata":{"papermill":{"duration":0.100308,"end_time":"2021-03-08T07:59:05.57403","exception":false,"start_time":"2021-03-08T07:59:05.473722","status":"completed"},"tags":[],"id":"s6J79vr7cpZY","outputId":"9c5ad330-a676-464d-e405-90c910908795","execution":{"iopub.status.busy":"2022-04-03T18:33:19.645295Z","iopub.execute_input":"2022-04-03T18:33:19.645987Z","iopub.status.idle":"2022-04-03T18:33:37.772288Z","shell.execute_reply.started":"2022-04-03T18:33:19.645946Z","shell.execute_reply":"2022-04-03T18:33:37.7715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = train['img_hog']\n\nnb_train_images = 80\n\n# lowers dimension from 3 to 2\ndef rgb2gray(rgb):\n    r, g, b = rgb[:,:,0], rgb[:,:,1], rgb[:,:,2]\n    gray = 0.2989 * r + 0.5870 * g + 0.1140 * b\n\n    return gray\n\nM = np.array(np.zeros([nb_train_images,10000]))\nfor n in range(nb_train_images):\n    Im = train_X[train_y][n]\n    Im = rgb2gray(Im)\n    A = np.asarray(Im).reshape(-1)\n    M[n,:] = A\n\n\ntsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300, random_state=1)\n\ntsne_results = tsne.fit_transform(M)\nTSNE1 = tsne_results[:,0].tolist()\nTSNE2 = tsne_results[:,1].tolist()\n\nsns.scatterplot(\n    x=TSNE1, y=TSNE2, hue=train['class'], palette=sns.color_palette(\"hls\", 3), data=df, legend=\"full\"\n)\n","metadata":{"id":"riKYqk6EmZI3","outputId":"7020b87d-7a1d-4b20-9de9-8b5622ae1aba","execution":{"iopub.status.busy":"2022-04-03T18:33:37.773465Z","iopub.execute_input":"2022-04-03T18:33:37.773844Z","iopub.status.idle":"2022-04-03T18:33:38.722948Z","shell.execute_reply.started":"2022-04-03T18:33:37.773812Z","shell.execute_reply":"2022-04-03T18:33:38.721886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.1.2. Discussion\n#### a) How to make your descriptor behave well in different circumstances (e.g. image with different lighting)?\nBy simply **normalizing** the images with a 'L2' parameter, we **avoid brightness, contrast or other illumination effects**. This was done by adding `block_norm= ‘L2’` as a parameter of the HOG feature extractor. We could also have used L1 and L2-Hys as volunteering parameters. L2-Hys is sometimes used to reduce noise with the help of a L2-norm.\n\n\n#### b) Visualize Feature Representation: [TODO]\nt-SNE : This method classifies the images into clusters with similarities. Given N points -  $x_1,x_2,…,x_N$ in any dimension, t-SNE will compute the probability  pi,\nj  the probability of similarity of  $x_i$  and  $x_j$ . The similarity is calculated based on the t-distribution. \nIn practice given a cluster of N points t-SNE helps in finding clusters in which there exists similarity in the data points.\n\nThe parameters for t-SNE are number of components =2, perplexity=40 which governs the number of nearest neighbours, learning rate=200, if this is too high the data will look like a ball with equispaced nearest neighbours.     \nnumber of iterations for optimization = 300. We also chose verbose = 1. The other various parameters were chosen to be default namely early_exaggeration, n_iter_without_progress, min_grad_norm,random_state=None, method='barnes_hut'.\n\n\n#### c) How does this feature compare to your previous grabbing task in the individual assignment?\nThe HOG feature extractor is a gradient based method and is generally known as a more robust method than the one we used in the individual assignment. In the individual assignment, the object grabbing was based on a **threshold of a particular colour**. The HOG feature extractor in contrary, uses the **magnitude and angle gradient** to compute features. It is computed by dividing an entire image into smaller cells an summing up their gradients over every pixel within each cell in an image.\n\n\n#### d) Did you need specific pre-processing steps before computing these feature descriptors on your images (which ones and why)?\nFirst of all, we convert the images to a **grayscale** as a basement for the following steps. Then, before processing, the images have been **resized**. In the paper of Dalal and Triggs, dimensions 128x64 were advised. However, the size depends on the application. In the notebook, the images were already resized to 100x100 pixels. We decided to resize the images to 80x80, so it would divisible by 4, 8 and 16. \nFurther, we **normalized** the images with a 'L2' parameter, **to avoid brightness, contrast or other illumination effects**. Other possible paramaters we could have used are L1 and L2-Hys. Finally, in order to take a few neighboring pixels into account instead of single pixel values as its true value, we performed a **Gaussian filtering**.\n\n#### e) Did the visualisation show good discriminative and robustness properties? [TODO]\nYes the t-SNE is able to show 3 major clusters of Jesse, Mila and the look alike. The HOG is able to show the major features of a face like nose, boundary, eyes etc. This is only possible for \nimages with distinct values in pixels. For the black images which are outliers this is not possible.\n\n\n#### f) Discussing missing values\n\nIn class 1 and 2, several black images appear. This means that the black images only contain 0 values, thus no information. Consequently, we can remove those images since they do not help our algorithm in classifying the faces correctly. \n\nThereby, a few images which do not contain a face, are added to the training set.\nIn statistics, it is advised noy to throw away outliers unless we surely know that they are measurement errors. This is why we decided to keep these images in our dataset. Thereby, they can probably help us not to overfit certain parameters of a face. ","metadata":{"papermill":{"duration":0.100596,"end_time":"2021-03-08T07:59:05.775686","exception":false,"start_time":"2021-03-08T07:59:05.67509","status":"completed"},"tags":[],"id":"Fc-R8QWtcpZY"}},{"cell_type":"markdown","source":"## 1.2. Baseline 2: PCA feature extractor\n...","metadata":{"papermill":{"duration":0.101426,"end_time":"2021-03-08T07:59:05.978236","exception":false,"start_time":"2021-03-08T07:59:05.87681","status":"completed"},"tags":[],"id":"wdRs0-gEcpZZ"}},{"cell_type":"code","source":"class PCAFeatureExtractor(IdentityFeatureExtractor):\n    \"\"\"TODO: this feature extractor is under construction\"\"\"\n    \n    def __init__(self, n_components):\n        self.n_components = n_components\n        \n    def transform(self, X):\n        raise NotImplmentedError\n        \n    def inverse_transform(self, X):\n        raise NotImplmentedError","metadata":{"papermill":{"duration":0.111032,"end_time":"2021-03-08T07:59:06.191215","exception":false,"start_time":"2021-03-08T07:59:06.080183","status":"completed"},"tags":[],"id":"8t7u3DxHcpZZ","execution":{"iopub.status.busy":"2022-04-03T18:33:38.724303Z","iopub.execute_input":"2022-04-03T18:33:38.724676Z","iopub.status.idle":"2022-04-03T18:33:38.731746Z","shell.execute_reply.started":"2022-04-03T18:33:38.724625Z","shell.execute_reply":"2022-04-03T18:33:38.730492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.2.1. Eigenface Plots\n...","metadata":{"papermill":{"duration":0.100881,"end_time":"2021-03-08T07:59:06.392861","exception":false,"start_time":"2021-03-08T07:59:06.29198","status":"completed"},"tags":[],"id":"JmAxCJpPcpZZ"}},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.100638,"end_time":"2021-03-08T07:59:06.595002","exception":false,"start_time":"2021-03-08T07:59:06.494364","status":"completed"},"tags":[],"id":"f55n04nccpZZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.2.2. Feature Space Plots\n...","metadata":{"papermill":{"duration":0.101263,"end_time":"2021-03-08T07:59:06.797448","exception":false,"start_time":"2021-03-08T07:59:06.696185","status":"completed"},"tags":[],"id":"sA59qKlMcpZZ"}},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.101801,"end_time":"2021-03-08T07:59:07.000598","exception":false,"start_time":"2021-03-08T07:59:06.898797","status":"completed"},"tags":[],"id":"eWOT5bFacpZZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.2.3. Discussion\n...","metadata":{"papermill":{"duration":0.102099,"end_time":"2021-03-08T07:59:07.204783","exception":false,"start_time":"2021-03-08T07:59:07.102684","status":"completed"},"tags":[],"id":"POCbXIRtcpZa"}},{"cell_type":"markdown","source":"# 2. Evaluation Metrics\n## 2.0. Example: Accuracy\nAs example metric we take the accuracy. Informally, accuracy is the proportion of correct predictions over the total amount of predictions. It is used a lot in classification but it certainly has its disadvantages...","metadata":{"papermill":{"duration":0.10088,"end_time":"2021-03-08T07:59:07.406787","exception":false,"start_time":"2021-03-08T07:59:07.305907","status":"completed"},"tags":[],"id":"dOPll75KcpZa"}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score","metadata":{"papermill":{"duration":1.180116,"end_time":"2021-03-08T07:59:08.688561","exception":false,"start_time":"2021-03-08T07:59:07.508445","status":"completed"},"tags":[],"id":"z3kXS2I2cpZa","execution":{"iopub.status.busy":"2022-04-03T18:33:38.733392Z","iopub.execute_input":"2022-04-03T18:33:38.733764Z","iopub.status.idle":"2022-04-03T18:33:38.749776Z","shell.execute_reply.started":"2022-04-03T18:33:38.733719Z","shell.execute_reply":"2022-04-03T18:33:38.74858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Classifiers\n## 3.0. Example: The *'not so smart'* classifier\nThis random classifier is not very complicated. It makes predictions at random, based on the distribution obseved in the training set. **It thus assumes** that the class labels of the test set will be distributed similarly to the training set.","metadata":{"papermill":{"duration":0.103749,"end_time":"2021-03-08T07:59:08.894358","exception":false,"start_time":"2021-03-08T07:59:08.790609","status":"completed"},"tags":[],"id":"J2SttRvxcpZa"}},{"cell_type":"code","source":"class RandomClassificationModel:\n    \"\"\"Random classifier, draws a random sample based on class distribution observed \n    during training.\"\"\"\n    \n    def fit(self, X, y):\n        \"\"\"Adjusts the class ratio instance variable to the one observed in y. \n\n        Parameters\n        ----------\n        X : tensor\n            Training set\n        y : array\n            Training set labels\n\n        Returns\n        -------\n        self : RandomClassificationModel\n        \"\"\"\n        \n        self.classes, self.class_ratio = np.unique(y, return_counts=True)\n        self.class_ratio = self.class_ratio / self.class_ratio.sum()\n        return self\n        \n    def predict(self, X):\n        \"\"\"Samples labels for the input data. \n\n        Parameters\n        ----------\n        X : tensor\n            dataset\n            \n        Returns\n        -------\n        y_star : array\n            'Predicted' labels\n        \"\"\"\n\n        np.random.seed(0)\n        return np.random.choice(self.classes, size = X.shape[0], p=self.class_ratio)\n    \n    def __call__(self, X):\n        return self.predict(X)\n    ","metadata":{"papermill":{"duration":0.113194,"end_time":"2021-03-08T07:59:09.110222","exception":false,"start_time":"2021-03-08T07:59:08.997028","status":"completed"},"tags":[],"id":"79ZNndbQcpZa","execution":{"iopub.status.busy":"2022-04-03T18:33:38.750928Z","iopub.execute_input":"2022-04-03T18:33:38.752141Z","iopub.status.idle":"2022-04-03T18:33:38.767853Z","shell.execute_reply.started":"2022-04-03T18:33:38.752014Z","shell.execute_reply":"2022-04-03T18:33:38.76669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.1. Baseline 1: My favorite classifier\n...","metadata":{"papermill":{"duration":0.101099,"end_time":"2021-03-08T07:59:09.31402","exception":false,"start_time":"2021-03-08T07:59:09.212921","status":"completed"},"tags":[],"id":"_SHzWmtUcpZb"}},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier","metadata":{"execution":{"iopub.status.busy":"2022-04-03T18:33:38.769359Z","iopub.execute_input":"2022-04-03T18:33:38.769757Z","iopub.status.idle":"2022-04-03T18:33:38.792052Z","shell.execute_reply.started":"2022-04-03T18:33:38.769711Z","shell.execute_reply":"2022-04-03T18:33:38.791201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = data[[\"img_hog\"]]\ny = data[[\"class\"]]\n\nnb_train_images = 80\n\n# 1 Train-validation set\nX_train, X_test, y_train, y_test = train_test_split(prepro(), y, test_size=0.2, random_state=0)\n\n# show how it looks like\nprint('Train set:', X_train.shape)\nprint('Test set:', X_test.shape)\n\nprint(X.shape)\nprint(y.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = data[[\"img_hog\"]]\ny = data[[\"class\"]]\n\n\n\ndef prepro():\n    nb_train_images = 80\n    \n    M = np.array(np.zeros([nb_train_images,10000]))\n    for n in range(nb_train_images):\n        Im = train_X[train_y][n]\n        Im = rgb2gray(Im)\n        A = np.asarray(Im).reshape(-1)\n        M[n,:] = A\n        return M\n\n# Full data: name, class, img, img_hog\n#data = train\n#print(data)\n\n# 1 Train-validation set\nX_train, X_test, y_train, y_test = train_test_split(prepro(), y, test_size=0.2, random_state=0)\n\nprint('Train set:', X_train.shape)\nprint('Test set:', X_test.shape)\n\nprint(X.shape)\nprint(y.shape)\n\nDT_model = DecisionTreeClassifier()\n\nDT_model.fit(X_train, y_train)\nDT_predict = DT_model.predict(X_test)\nDT_score = DT_model.score(X_train,y_train)\n\nDT_score","metadata":{"execution":{"iopub.status.busy":"2022-04-03T19:03:10.581476Z","iopub.execute_input":"2022-04-03T19:03:10.582313Z","iopub.status.idle":"2022-04-03T19:03:10.624483Z","shell.execute_reply.started":"2022-04-03T19:03:10.582271Z","shell.execute_reply":"2022-04-03T19:03:10.623433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DecisionTree:\n    \"\"\"TODO: this classifier is under construction.\"\"\"\n    def __init__(self):\n        self.model = DecisionTreeClassifier()\n        \n    def fit(self, X, y):\n        self.model.fit(X, y)\n                \n    def predict(self, X):\n        self.predict = self.model.predict(X)\n        \n    def score(self, X, y):\n        return self.model.score(X, y)","metadata":{"papermill":{"duration":0.108542,"end_time":"2021-03-08T07:59:09.525054","exception":false,"start_time":"2021-03-08T07:59:09.416512","status":"completed"},"tags":[],"id":"kOkaoSQlcpZb","execution":{"iopub.status.busy":"2022-04-03T19:08:58.238429Z","iopub.execute_input":"2022-04-03T19:08:58.239018Z","iopub.status.idle":"2022-04-03T19:08:58.245546Z","shell.execute_reply.started":"2022-04-03T19:08:58.238977Z","shell.execute_reply":"2022-04-03T19:08:58.244631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DT = DecisionTree()\nDT.fit(X_train, y_train)\nDT.predict(X_test)\nDT.score(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-04-03T19:14:20.740068Z","iopub.execute_input":"2022-04-03T19:14:20.74085Z","iopub.status.idle":"2022-04-03T19:14:20.760399Z","shell.execute_reply.started":"2022-04-03T19:14:20.740793Z","shell.execute_reply":"2022-04-03T19:14:20.759771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\n\nX = data[[\"img_hog\"]]\ny = data[[\"class\"]]\n\nnb_train_images = 80\n\ndef prepro():\n    M = np.array(np.zeros([nb_train_images,10000]))\n    for n in range(nb_train_images):\n        Im = train_X[train_y][n]\n        Im = rgb2gray(Im)\n        A = np.asarray(Im).reshape(-1)\n        M[n,:] = A\n        return M\n\n# Full data: name, class, img, img_hog\ndata = train\n#print(data)\n\n# 1 Train-validation set\nX_train, X_test, y_train, y_test = train_test_split(prepro(), y, test_size=0.2, random_state=0)\n\nprint('Train set:', X_train.shape)\nprint('Test set:', X_test.shape)\n\nprint(X.shape)\nprint(y.shape)\n\nDT_model = DecisionTreeClassifier()\n\nDT_model.fit(X_train, y_train)\nDT_predict = DT_model.predict(X_test)\nDT_score = DT_model.score(X_train,y_train)\n\nDT_score","metadata":{"execution":{"iopub.status.busy":"2022-04-03T18:47:22.365285Z","iopub.execute_input":"2022-04-03T18:47:22.365743Z","iopub.status.idle":"2022-04-03T18:47:22.40773Z","shell.execute_reply.started":"2022-04-03T18:47:22.365707Z","shell.execute_reply":"2022-04-03T18:47:22.406933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FavoriteClassificationModel:\n    \"\"\"TODO: this classifier is under construction.\"\"\"\n    \n    def fit(self, X, y):\n        raise NotImplmentedError\n        \n    def predict(self, X):\n        raise NotImplmentedError","metadata":{"execution":{"iopub.status.busy":"2022-04-03T18:33:39.195658Z","iopub.status.idle":"2022-04-03T18:33:39.196032Z","shell.execute_reply.started":"2022-04-03T18:33:39.195849Z","shell.execute_reply":"2022-04-03T18:33:39.195868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Experiments\n<div class=\"alert alert-block alert-info\"> <b>NOTE:</b> Do <i>NOT</i> use this section to keep track of every little change you make in your code! Instead, highlight the most important findings and the major (best) pipelines that you've discovered.  \n</div>\n<br>\n\n## 4.0. Example: basic pipeline\nThe basic pipeline takes any input and samples a label based on the class label distribution of the training set. As expected the performance is very poor, predicting approximately 1/4 correctly on the training set. There is a lot of room for improvement but this is left to you ;). ","metadata":{"papermill":{"duration":0.102942,"end_time":"2021-03-08T07:59:09.730342","exception":false,"start_time":"2021-03-08T07:59:09.6274","status":"completed"},"tags":[],"id":"zirrcsJ3cpZb"}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\n# FEATURE EXTRACTOR WILL SELECT A FEW FEATURES (..) ON WHICH WE WILL TRAIN. NOW\n# WWe are training on all the data.\nfeature_extractor = IdentityFeatureExtractor() \n\nclassifier = DecisionTree()\n\nclassifier.fit(X_train, y_train)\n#classifier.predict(X_test)\n#classifier.score(X_train, y_train)\n\n#print(classifier)\n\n# train the model on the features\n#classifier.fit(feature_extractor(train_X), train_y)\nclassifier.fit(feature_extractor(X_train), y_train)\n#classifier.fit(M, train_y)\n\n# model/final pipeline\nmodel = lambda X: classifier(feature_extractor(X))","metadata":{"papermill":{"duration":0.430319,"end_time":"2021-03-08T07:59:10.263691","exception":false,"start_time":"2021-03-08T07:59:09.833372","status":"completed"},"tags":[],"id":"vRTLulaLcpZb","execution":{"iopub.status.busy":"2022-04-03T19:16:59.496262Z","iopub.execute_input":"2022-04-03T19:16:59.496631Z","iopub.status.idle":"2022-04-03T19:16:59.52693Z","shell.execute_reply.started":"2022-04-03T19:16:59.496593Z","shell.execute_reply":"2022-04-03T19:16:59.526165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# evaluate performance of the model on the training set\n#train_y_star = model(train_X)\ntrain_y_star = model(X_train)\n\n\"The performance on the training set is {:.2f}. This however, does not tell us much about the actual performance (generalisability).\".format(\n    accuracy_score(train_y, train_y_star))","metadata":{"papermill":{"duration":0.114717,"end_time":"2021-03-08T07:59:10.480473","exception":false,"start_time":"2021-03-08T07:59:10.365756","status":"completed"},"tags":[],"id":"au4888SocpZb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predict the labels for the test set \ntest_y_star = model(X_test)","metadata":{"papermill":{"duration":0.111828,"end_time":"2021-03-08T07:59:10.696438","exception":false,"start_time":"2021-03-08T07:59:10.58461","status":"completed"},"tags":[],"id":"D54Icb6vcpZc","execution":{"iopub.status.busy":"2022-04-03T19:16:29.058317Z","iopub.execute_input":"2022-04-03T19:16:29.058666Z","iopub.status.idle":"2022-04-03T19:16:29.107933Z","shell.execute_reply.started":"2022-04-03T19:16:29.05863Z","shell.execute_reply":"2022-04-03T19:16:29.106697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Publishing best results","metadata":{"papermill":{"duration":0.103853,"end_time":"2021-03-08T07:59:10.903341","exception":false,"start_time":"2021-03-08T07:59:10.799488","status":"completed"},"tags":[],"id":"eEJfON27cpZc"}},{"cell_type":"code","source":"submission = test.copy().drop('img', axis = 1)\nsubmission['class'] = test_y_star\n\nsubmission","metadata":{"papermill":{"duration":0.120392,"end_time":"2021-03-08T07:59:11.127762","exception":false,"start_time":"2021-03-08T07:59:11.00737","status":"completed"},"tags":[],"id":"DMGtEzjZcpZc","execution":{"iopub.status.busy":"2022-04-03T18:33:39.202982Z","iopub.status.idle":"2022-04-03T18:33:39.203371Z","shell.execute_reply.started":"2022-04-03T18:33:39.203164Z","shell.execute_reply":"2022-04-03T18:33:39.203182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv')","metadata":{"papermill":{"duration":0.122516,"end_time":"2021-03-08T07:59:11.356409","exception":false,"start_time":"2021-03-08T07:59:11.233893","status":"completed"},"tags":[],"id":"yADy_VNRcpZc","execution":{"iopub.status.busy":"2022-04-03T18:33:39.204885Z","iopub.status.idle":"2022-04-03T18:33:39.205278Z","shell.execute_reply.started":"2022-04-03T18:33:39.205078Z","shell.execute_reply":"2022-04-03T18:33:39.205098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Discussion\n...\n\nIn summary we contributed the following: \n* \n","metadata":{"papermill":{"duration":0.116655,"end_time":"2021-03-08T07:59:11.577703","exception":false,"start_time":"2021-03-08T07:59:11.461048","status":"completed"},"tags":[],"id":"2h1EsEh7cpZd"}}]}